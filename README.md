# NHL Game Prediction in R
The objective of this project is to develop a machine learning model that accurately predicts winning NHL teams. The aim is to provide insights into the factors that influence winning in the NHL and to demonstrate the potential of machine learning models in predicting sports outcomes. 
# Data
The data for this study was obtained from the NHL Game Data available on Kaggle. The data consisted of originally thirteen datasets, each including separate data based off game history, player history, or team history. Due to the project, only data related to game prediction was used, and since this data consists of NHL game season data from the 2007/2008 season to the 2017/2018 season, all player information datasets were removed since players are traded between teams and seasons and their stats are not relevant to the project. In the end, the two datasets, game.csv and game_teams_stats.cav were used for the project.

The game.csv dataset contained the following variables: game_id, season, type, date_time, away_team_id, home_team_id, away_goals, home_goals, outcome, and home_rink_side_start. The variable "type" was used to determine whether a game was a regular season game or a playoff game. The game_teams_stats.csv dataset contained the following variables: game_id, team_id, HoA, won, settled_in, head_coach, goals, shots, hits, pim, powerPlayOpportunities, powerPlayGoals, faceOffWinPercentage, giveaways, takeaways, blocked, and startRinkSide.
After importing the game.csv dataset, I selected the 'type' variable, which denotes whether a game is played in the regular season or playoffs, as well as the game_ID variable. The game_ID variable was used to merge the game.csv dataset with the game_teams_stats.csv dataset. All character variables were then converted to categorical variables to prepare the data for modeling. In addition, the 'coach' variable was dropped from the dataset, as coaches fall under the same category as players and are not relevant for the project.

Next, missing variables were identified and addressed. The variable with the most missing values was 'faceOffWinPercentage', which was dealt with by setting the missing values to the mean of the non-missing values. Four other variables, 'hits', 'giveaways', 'blocked', and 'takeaways', all had the same 4928 missing observations. These observations were removed from the dataset, leaving no missing values in the final dataset. These data cleaning steps ensured that the data was ready for exploratory analysis and modeling.

# Exploratory Analysis
![image](https://user-images.githubusercontent.com/50085554/236952222-d3b6f02d-7c58-4878-9655-cc0c1973141c.png)
The above correlation plot shows the correlation between each variable. Since the variable, startRinkSide has no correlation with the won variable it has no prediction power and will be dropped from the model. Now I have determined which variables are most important for predicting which team will win. The data is now split into testing and training data and model building is started. 

Starting with the XGBoost model, the training data is split into the data portion consisting of a matrix version of the predictor variables. Then a vector of the prediction variable is created, won. The XGBoost function is used on this data now, it is important to note that the computation time for building this model is considerably faster than most machine learning techniques. This model is then used to predict on the testing data, and a RMSE of 0.351 is found.

Next, a random forest model will be built based off the training data. A total of 500 trees are used. Then the testing data is used to predict on, and finally a RMSE of 0.648 is found based off the predicted values. The computation time for the random forest model is considerably more than the XGBoost model, this model also performed worse than the XGBoost model, thus it is not the best model to use for this data.

Finally, an SVM model is built using the training data. Since the classes are linearly separable, the linear kernel is used. This SVM model is then used to predict on the test data, and a RMSE of 0.398 is found for this model. The computation time for this model was the longest out of all the models built. Whilst the RMSE of this model was better than the RMSE for the random forest model, it still is not better than the XGBoost model. Thus, the XGBoost model is determined to be the best model to use.

# Final Analysis
Through a comprehensive analysis of the initial data, I have identified the most relevant variables for building a machine learning model to accurately predict hockey team wins. This project's goal was to develop a model that would provide valuable insights into the factors that influence winning in the NHL and demonstrate the potential of machine learning models in predicting sports outcomes.
To achieve this, I have created three distinct machine learning models, utilizing support vector machines, random foresting, and XGBoosting. Each model was carefully designed to predict hockey team wins based on the teams playing, while also taking into account a variety of key factors that could potentially impact the outcome of the game.
After training and testing each of these models, I found that the XGBoost model was the clear standout, exhibiting unparalleled efficiency and accuracy compared to the other models. The XGBoost model was able to accurately predict hockey team wins, utilizing advanced techniques such as gradient boosting and feature selection to improve the model's accuracy and precision. This model also demonstrated the ability to identify the most significant factors that influence team wins, enabling us to provide valuable insights to coaches, players, and team managers looking to improve their team's performance through data-driven decision making.

